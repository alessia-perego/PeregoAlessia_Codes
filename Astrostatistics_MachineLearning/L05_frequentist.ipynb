{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classical/Frequentist Statistical Inference: I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Inference\n",
    "\n",
    "Statistical *inference* is about drawing conclusions from data, specifically determining the properties of a population by data sampling.\n",
    "\n",
    "Three examples of inference are:\n",
    "1. What is the best estimate for a model parameter?\n",
    "2. How confident we are about our result?\n",
    "3. Are the data consistent with a particular model/hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Terminology\n",
    "\n",
    "* We typically study the properties of some ***population*** by measuring ***samples*** from that population. The population doesn't have to refer to different objects. E.g., we may be (re)measuring the position of an object at rest; the population is the distribution of (an infinite number of) measurements smeared by the uncertainty, and the sample are the measurement we've actually taken.\n",
    "\n",
    "\n",
    "* A ***statistic*** is any function of the sample. For example, the sample mean is a statistic, but also \"the value of the first measurement\".\n",
    "\n",
    "* To conclude something about the population from the sample, we develop ***estimators***. An estimator is a statistic, a rule for calculating an estimate of a given quantity based on observed data.\n",
    "\n",
    "* There are ***point*** and ***interval estimators***. The point estimators yield single-valued results (example: position of an object), while with an interval estimator, the result would be a range of plausible values (example: confidence interval for the position of an object).\n",
    "\n",
    "* Measurements have **uncertainties** (not errors) and we need to account for these (sometimes they are unknown).\n",
    "\n",
    "* Data are not variables but fixed values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Frequentist vs. Bayesian Inference\n",
    "\n",
    "There are two major statistical paradigms that address the statistical inference questions: \n",
    "- the **classical**, or **frequentist** paradigm,\n",
    "- the **Bayesian** paradigm.\n",
    "\n",
    "While most of statistics and machine learning is based on the classical paradigm, Bayesian techniques are being embraced by the statistical and scientific communities at an ever-increasing pace...especially in astrophysics.\n",
    "\n",
    "#### Key differences\n",
    "- **Definition of probabilities**:\n",
    "    - In ***frequentist inference***, probabilities describe the ***relative frequency of events*** over repeated experimental trials. \n",
    "    - In ***Bayesian inferenece***, probabilities instead quantify our ***subjective belief about experimental outcomes, model parameters, or even models themselves***. \n",
    "    \n",
    "    \n",
    "- **Quantifying uncertainty**:\n",
    "    - In ***frequentist inference*** we have ***confidence levels*** that describe the distribution of the measured parameter from the data around the true value.\n",
    "    - In ***Bayesian inference*** we have ***credible regions*** derived from posterior probabilitiy distributions. These encode our \"***belief spread***\" in model parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result is that Alice's is not 160, but rather 148! More specifically, my Bayesian measurement is that $p(141.3\\le \\mu \\le 154.7 \\, | \\, \\overline{x}=160) = 0.683$ (i.e. there's a probability of 68% that Alice's IQ is between 141.3 and 154.7). This estimate incorporates not only the test I made to that person, but also the prior information of how the IQ distribution is calibrated. We already saw the huge influence that priors can have and why considering priors is very reasonable.\n",
    "\n",
    "This all seems totally fine; where's the controvery with Bayesian methods? If you go through with this idea,  everything (but data!) needs to be promoted to a random variable, incluing a theory (what does it mean that the Standard Model of Particle Physics is a random variable?) or a constant of nature (what does it mean that the speed of light is a random variable?). We'll return to this in a few lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximum Likelihood Estimation (MLE) <a class=\"anchor\" id=\"two\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Maximum Likelihood Approach\n",
    "\n",
    "Maximum likelihood estimation follows this blueprint:\n",
    "\n",
    "1. **Hypothesis**: Formulate a model, a *hypothesis*, about how the data are generated. For example, data are a measurement of some quantity with Gaussian random uncertainties. Models are typically described using a set of model parameters $\\boldsymbol{\\theta}$, and written as $\\boldsymbol{M}(\\boldsymbol{\\theta})$.\n",
    "\n",
    "\n",
    "2. **Maximum Likelihood Estimation**: Search for the \"best\" model parameters $\\boldsymbol{\\theta}$ which maximize the ***likelihood*** $L(\\boldsymbol{\\theta}) \\equiv p(D|M(\\theta))$. This search yields the MLE *point estimates*, $\\boldsymbol{\\theta^0}$.\n",
    "\n",
    "\n",
    "3. **Quantifying Estimate Uncertainty**: Determine the confidence region for model parameters, $\\boldsymbol{\\theta^0}$. Such a confidence estimate can be obtained analytically (possibly with some approximations), but can also be done numerically for arbitrary models using general frequentist techniques, such as bootstrap, jackknife, and cross-validation.\n",
    "\n",
    "\n",
    "4. **Hypothesis Testing**: Perform hypothesis tests as needed to make other conclusions about models and point estimates. Possibly GOTO #1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example: Measuring the Position of a Quasar\n",
    "\n",
    "Let's assume we wish to estimate the position $x$ of a quasar from a series of individual astrometric measurements.\n",
    "\n",
    "1. We adopt a model where the observed quasar does not move, and has individual measurement uncertainties \n",
    "2. We derive the expression for the likelihood of there being a quasar at position $x_0$ that gives rise to our individual measurements. We find the value of $\\hat x_0$ for which our observations are maximally likely.\n",
    "3. We determine the uncertainties (confidence intervals) on our measurement.\n",
    "4. We test whether what we've observed is consistent with our adopted model. For example, is it possible that the quasar was really a misidentified star with measurable proper motion?\n",
    "\n",
    "Note: in the text to come, I will use $\\mu$ instead of $x_0$ to denote the true position of the quasar. This is to avoid potential confusion with the first (or zeroth) measurement of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The Likelihood Function\n",
    "\n",
    "\n",
    "If we know the distribution from which our data were drawn (or make a hypothesis about it), then we can compute the **probability** of our data being generated.\n",
    "\n",
    "Another way of saying the same thing: **the likelihood is your model for the process that generates the data.**\n",
    "\n",
    "Therefore, the likelihood is a function of the data given the parameters (not the other way round).\n",
    "\n",
    "For example, if our data are generated by a Gaussian process with mean $\\mu$ and standard deviation $\\sigma$, then the probability density of a certain value $x$ is\n",
    "\n",
    "$$p(x|\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x-\\mu)^2}{2\\sigma^2}\\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If we want to know the total probability of our ***entire*** data set (as opposed to one measurement) then we must compute the ***product*** of all the individual probabilities:\n",
    "\n",
    "$$L \\equiv p(\\{x_i\\}|M(\\theta)) = \\prod_{i=1}^N p(x_i|M(\\theta)),$$\n",
    "\n",
    "where $M$ is the *model* and $\\theta$ refers collectively to the $k$ parameters of the model, which can generally be multi-dimensional. In words...\n",
    "\n",
    "> $L(\\{x_i\\})\\equiv$ the probability of the data given the model parameters. \n",
    "\n",
    "If we consider $L$ as a function of the model parameters, we refer to it as\n",
    "\n",
    "> $L(\\theta)\\equiv$ likelihood of the model parameters, given the observed data. \n",
    "\n",
    "Here: **we're thinking about the probability of the data as a function of the model parameters.** This is the crucial point behind MLE (and inference in general).\n",
    "\n",
    "Note:\n",
    "- [Jeynes](https://www.amazon.com/Probability-Theory-Science-T-Jaynes/dp/0521592712) is quite strict on how refer to the likelihood of model parameters versus the probability of the data.\n",
    "- while the components of $L$ may be normalized pdfs, their product is not.\n",
    "- the product can be very small, so we often take the log of $L$. \n",
    "- we're assuming the individual measurements are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can write $L$ out as\n",
    "\n",
    "$$L = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right),$$\n",
    "\n",
    "and simplify to\n",
    "\n",
    "$$L = \\left( \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right),$$\n",
    "\n",
    "where we have written the product of the exponentials as the exponential of the sum of the arguments, which will make things easier to deal with later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you have done $\\chi^2$ analysis (e.g., doing a linear least-squares fit), then you might notice that the argument of the exponential is just \n",
    "\n",
    "$$\\exp \\left(-\\frac{\\chi^2}{2}\\right).$$\n",
    "\n",
    "That is, for our gaussian distribution\n",
    "\n",
    "$$\\chi^2 = \\sum_{i=1}^N \\left ( \\frac{x_i-\\mu}{\\sigma}\\right)^2.$$\n",
    "\n",
    "So for Gaussians **maximizing the likelihood or log-likelihood is the same as minimizing $\\chi^2$**.  In both cases we are finding the most likely values of our model parameters (here $\\mu$ and $\\sigma$).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Properties of ML Estimators\n",
    "\n",
    "Assuming the data truly are drawn from the model, ML estimators have the following useful properties:\n",
    "\n",
    "* **They are consistent estimators**. They converge to the true parameter value as $N\\to\\infty$.\n",
    "\n",
    "\n",
    "* **They are asymptotically normal estimators**. As $N\\to\\infty$ the distribution of the parameter estimate approaches a normal distribution, centered at the MLE, with a certain spread.\n",
    "\n",
    "\n",
    "* **They asymptotically achieve the theoretical minimum possible variance, called the Cramér–Rao bound**. They achieve the best possible uncertainty given the data at hand; no other estimator can do better in terms of efficiently using each data point to reduce the total error of the estimate (see eq. 3.33 in the textbook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is why you've been doing $\\chi^2$ all the time. You want a MLE, which is the same as minimizing $\\chi^2$ ***if*** data are Gaussian. This is reasonable because of the Central Limit Theorem, but not always true! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The key idea behind Maximum Likelihood Estimation\n",
    "\n",
    "Let's say that we know that some data were drawn from a Gaussian distribution, but we don't know the $\\theta = (\\mu,\\sigma)$ values of that distribution (i.e., the parameters).\n",
    "\n",
    "Then Maximum Likelihood Estimation method tells us to think of the likelihood as a ***function of the unknown model parameters***, and to ***find the parameters that maximize the value of $L$***. Those will be our *Maximum Likelihood Estimators* for for the true values of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLE applied to a homoscedastic Gaussian <a class=\"anchor\" id=\"three\"></a>\n",
    "\n",
    "Let's take a look at our astrometry example, using a model where all the measurements have the same uncertainty, drawn from a normal distribution, $N(0, \\sigma)$.\n",
    "\n",
    "As mentioned back in our early lectures, uncertainties being the same is known as having **homoscedastic** uncertainties which just means \"uniform uncertainties\".  Later we will consider the case where the measurements can have different uncertainties ($\\sigma_i$) which is called **heteroscedastic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have an experiment with the set of measured positions $D=\\{x_i\\}$ in 1D with Gaussian uncertainties, and therefore:\n",
    "\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu,\\sigma) = \\prod_{i=1}^N \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(\\frac{-(x_i-\\mu)^2}{2\\sigma^2}\\right).$$\n",
    "\n",
    "Note that that is $p(\\{x_i\\})$ not $p(x_i)$, that is the probability of the full data set, not just one measurement. If $\\sigma$ is both constant and *known*, then this is a one parameter model with $k=1$ and $\\theta_1=\\mu$. \n",
    "\n",
    "We're assuming we know the spread of the measurments $\\sigma$ here (say it depends on the properties of the telescope), but of course that's not always the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we found above, likelihoods can be really small, so let's define the **log-likelihood function** as ${\\ln L} = \\ln[L(\\theta)]$.  The maximum of this function happens at the same place as the maximum of $L$.  Note that any constants in $L$ have the same effect for all model parameters, so constant terms can be ignored.  \n",
    "\n",
    "In this case we then have \n",
    "\n",
    "$${\\rm lnL} = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma^2}.$$\n",
    "\n",
    "Above, we wrote\n",
    "\n",
    "$$L = \\prod_{i=1}^N \\left( \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right) \\exp\\left( -\\frac{1}{2} \\sum \\left[\\frac{-(x_i-\\mu)}{\\sigma} \\right]^2 \\right).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We then determine the maximum.  It is the parameter set for which the derivative of ${\\rm lnL}$ is zero:\n",
    "\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\hat \\mu} \\equiv 0.$$\n",
    "\n",
    "That gives $$ \\sum_{i=1}^N \\frac{(x_i - \\hat \\mu)}{\\sigma^2} = 0.$$\n",
    "\n",
    "Note: \n",
    "- We should also check that the $2^{\\rm nd}$ derivative is negative, to ensure this is the *maximum* of $L$.\n",
    "- Bimodality? We need something more.\n",
    "- Any constants in $\\ln L$ disappear when differentiated, so constant terms can typically be ignored. This will change if we're trying to select between different models, rather than just parameter estimation within a single model as we're doing here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since $\\sigma = {\\rm constant}$ (not always, but here at least), that says \n",
    "\n",
    "$$\\sum_{i=1}^N x_i = \\sum_{i=1}^N \\hat \\mu = N \\hat \\mu.$$\n",
    "\n",
    "Thus we find that\n",
    "\n",
    "$$\\hat \\mu = \\frac{1}{N}\\sum_{i=1}^N x_i,$$\n",
    "\n",
    "***which is just the sample arithmetic mean of all the measurements!*** Thus **the sample mean is a ML estimator**. We got there in a roundabout way, but still pretty easy.\n",
    "\n",
    "So at the end of the day, if you know the $\\sigma$ of your quasar measuurements (perhaps because it comes from the property of the telescope), just compute a mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Quantifying estimate uncertainty <a class=\"anchor\" id=\"four\"></a>\n",
    "\n",
    "Our ML estimate of $\\mu$ is not perfect. The uncertaintly of the estimate is captured by the shape and distribution of the likelihood function, but we'd like to capture that with a few numbers.\n",
    "\n",
    "Note: $\\sigma$ is *not* the uncertanty on $\\mu$!\n",
    "\n",
    "The ***asymptotic normality of MLE*** is invoked to approximate the likelihood function as a Gaussian (or the $\\ln L$ as a parabola), i.e. we take a Taylor expansion around the MLE, keep terms up $2^\\mathrm{nd}$ order, then *define* the uncertainty on our model parameters as:\n",
    "\n",
    "$$\\sigma_{jk} = \\sqrt{[F^{-1}]_{jk}}, $$\n",
    "\n",
    "where\n",
    "\n",
    "$$ F_{jk} = - \\frac{d^2 \\ln L}{d\\theta_j d\\theta_k} \\Biggr\\rvert_{\\theta=\\hat \\theta}.$$\n",
    "\n",
    "The matrix $F$ is known as the **Fisher information matrix**.  The elements $\\sigma^2_{jk}$ are known as the ***covariance matrix***.\n",
    "\n",
    "The marginal error bars for each parameter, $\\theta_i$ are given by the diagonal elements, $\\sigma_{ii}$. These are the \"error bars\" that are typically quoted with each measurement. Off diagonal elements, $\\sigma_{ij}$, arise from any correlation between the parameters in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In our example of a homoscedastic Gaussian, the uncertainly on the mean is \n",
    "\n",
    "$$\\sigma_{\\mu} = \\left( - \\frac{d^2\\ln L(\\mu)}{d\\mu^2}\\Biggr\\rvert_{\\hat \\mu}\\right)^{-1/2}$$\n",
    "\n",
    "We find\n",
    "\n",
    "$$\\frac{d^2\\ln L(\\mu)}{d\\mu^2}\\Biggr\\rvert_{\\hat \\mu} = - \\sum_{i=1}^N\\frac{1}{\\sigma^2} = -\\frac{N}{\\sigma^2},$$\n",
    "\n",
    "since, again, $\\sigma = {\\rm constant}$.  \n",
    "\n",
    "Then \n",
    "\n",
    "$$\\sigma_{\\mu} = \\frac{\\sigma}{\\sqrt{N}}.$$\n",
    "\n",
    "So, our estimator of $\\mu$ is $\\overline{x}\\pm\\frac{\\sigma}{\\sqrt{N}}$, which is a result that you should be familiar with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The $(\\hat \\mu - \\sigma_\\mu, \\hat \\mu + \\sigma_\\mu)$ range gives us a **confidence interval**.\n",
    "\n",
    "In frequentist interptetation, if we repeated the same measurement a hundred times, we'd find for 68 experiments the true value was within their computed confidence intervals ($1 \\sigma$ errors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MLE applied to a Heteroscedastic Gaussian <a class=\"anchor\" id=\"five\"></a>\n",
    "\n",
    "Now let's look a case where the uncertainties are heteroscedastic.  For example if we are measuring the position of a quasar $N$ measurements, $\\{x_i\\}$, where the uncertainty for each measurement, $\\sigma_i$ is known (say the setups in your telescope changed: you know what they are, but they're different every night).  Since $\\sigma$ is not a constant, then following the above, we have\n",
    "\n",
    "$$\\ln L = {\\rm constant} - \\sum_{i=1}^N \\frac{(x_i - \\mu)^2}{2\\sigma_i^2}.$$\n",
    "\n",
    "Taking the derivative:\n",
    "$$\\frac{d\\;{\\rm lnL}(\\mu)}{d\\mu}\\Biggr\\rvert_{\\hat \\mu} = \\sum_{i=1}^N \\frac{(x_i - \\hat \\mu)}{\\sigma_i^2} = 0,$$\n",
    "then simplifying:\n",
    "\n",
    "$$\\sum_{i=1}^N \\frac{x_i}{\\sigma_i^2} = \\sum_{i=1}^N \\frac{\\hat \\mu}{\\sigma_i^2},$$\n",
    "\n",
    "yields a MLE solution of \n",
    "$$\\hat \\mu = \\frac{\\sum_i^N (x_i/\\sigma_i^2)}{\\sum_i^N (1/\\sigma_i^2)},$$\n",
    "\n",
    "with uncertainty\n",
    "$$\\sigma_{\\mu} = \\left( \\sum_{i=1}^N \\frac{1}{\\sigma_i^2}\\right)^{-1/2}.$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Working with non-Gaussian Likelihoods <a class=\"anchor\" id=\"six\"></a>\n",
    "\n",
    "As an example of MLE with non-Gaussian probability density we can use the same formalism above for a Poisson distribution. In this case we write the probability disrtibution as\n",
    "\n",
    "$$p(x_i|\\mu) = \\frac{e^{-\\mu}\\mu^{x_i}}{x_i!}$$\n",
    "\n",
    "with $\\mu$ the average number of events, $N$ is the number of observed events, and $\\{x_i\\}$ are the measured data.\n",
    "\n",
    "As we saw before, this distribution is particularly useful for characterizing the number of soldiers in the Prussian army killed accidentally by horse kicks. That is indeed a typical task in modern astrophysics research.\n",
    "\n",
    "We can then write the likelihood as\n",
    "\n",
    "$$L \\equiv p(\\{x_i\\}|\\mu) = \\prod_{i=1}^{N} \\frac{e^{-\\mu}\\mu^{x_i}}{x_i!}$$\n",
    "\n",
    "and the $\\ln L$ as\n",
    "\n",
    "$$\\ln L = \\sum_{i=1}^{N} \\ln \\bigg( \\frac{e^{-\\mu}\\mu^{x_i}}{x_i!} \\bigg)$$\n",
    "\n",
    "$$= \\sum_{i=1}^{N} -\\mu + x_i \\; \\ln(\\mu) - \\ln({x_i!})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximizing the $\\ln L$ \n",
    "\n",
    "For the Poisson distribution we can solve for the maximum liklehood analytically\n",
    "\n",
    "$$\\frac{\\partial \\; L(\\mu)}{\\partial \\; \\mu} = \\frac{\\partial \\; }{\\partial \\; \\mu} \\bigg( \\sum_{i=1}^{N} -\\mu + x_i \\; \\ln(\\mu)\\bigg)$$\n",
    "\n",
    "$$0 = \\sum_{i=1}^{N} \\bigg( -1 + \\frac{x_i}{\\mu} \\bigg)$$\n",
    "$$\\hat\\mu = \\frac{1}{N} \\sum_{i=1}^{N} x_i $$\n",
    "\n",
    "***For many likelihoods we cannot solve for the maximum analytically, and we have to resort to numerical solutions.***  (MCMC and robust statistics.)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "livereveal": {
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "sky"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
